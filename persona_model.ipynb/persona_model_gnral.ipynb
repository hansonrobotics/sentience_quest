{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Model with sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.10.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/mariorodriguez/HansonRobotics/pop/.conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.1 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.16.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.1-cp311-cp311-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, sniffio, pydantic-core, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.2.0 certifi-2023.11.17 distro-1.9.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 idna-3.6 openai-1.10.0 pydantic-2.6.0 pydantic-core-2.16.1 sniffio-1.3.0 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def extract_content_from_response(response):\n",
    "    # Comprobando si la respuesta y las elecciones están presentes\n",
    "    if response is not None and hasattr(response, 'choices') and len(response.choices) > 0:\n",
    "        # Accediendo al mensaje de la primera elección\n",
    "        chat_message = response.choices[0].message\n",
    "\n",
    "        # Comprobando si el mensaje no es nulo y tiene un contenido\n",
    "        if chat_message is not None and hasattr(chat_message, 'content'):\n",
    "            return chat_message.content\n",
    "        else:\n",
    "            return \"No content found in message.\"\n",
    "    else:\n",
    "        return \"No choices found in response.\"\n",
    "\n",
    "def sentiment_analysis(transcription):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"As an AI with expertise in language and emotion analysis, your task is to analyze the sentiment of the following text. Please consider the overall tone of the discussion, the emotion conveyed by the language used, and the context in which words and phrases are used. Indicate whether the sentiment is generally positive, negative, or neutral, and provide brief explanations for your analysis where possible. Your final output is a json format with a list of the analyzed sentiment in the first position, then the tone, and others goes in the next positions. Add an score in decimals from 0 to 1 of how impact does the sentiment is being detected in the text. Just add in the json: sentiment, score_of_sentiment, tone, score_of_tones, their content should be lists\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcription\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=1024,\n",
    "        top_p=0,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return extract_content_from_response(response)\n",
    "    \n",
    "    #return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"sentiment\": [\"positive\"],\n",
      "\"score_of_sentiment\": [0.9],\n",
      "\"tone\": [\"excited\", \"optimistic\"],\n",
      "\"score_of_tones\": [0.8, 0.7]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_analysis('AI future seems to be promising. I am excited to see what the future holds.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name, is_human):\n",
    "        self.name = name\n",
    "        self.is_human = is_human\n",
    "        self.emotions = {\n",
    "            \"positive\": 0,\n",
    "            \"negative\": 0,\n",
    "            \"neutral\": 0,\n",
    "        }\n",
    "\n",
    "    def update_emotion(self, emotion, value):\n",
    "        if emotion in self.emotions:\n",
    "            self.emotions[emotion] = value\n",
    "        else:\n",
    "            print(f\"'{emotion}' emotion not recognized.\")\n",
    "\n",
    "    def reflect_on_self(self): #get the emotions' values of the Person\n",
    "\n",
    "        reflection = {\n",
    "            \"positive\": [],\n",
    "            \"negative\": [],\n",
    "            \"neutral\": []\n",
    "        }\n",
    "\n",
    "        for emotion, value in self.emotions.items():\n",
    "            if emotion == \"positive\" and value > 0:\n",
    "                reflection[\"positive\"].append((value))\n",
    "            elif emotion == \"negative\" and value > 0:\n",
    "                reflection[\"negative\"].append((value))\n",
    "            else:\n",
    "                reflection[\"neutral\"].append((value))\n",
    "\n",
    "        return reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'positive': [0.7], 'negative': [0.3], 'neutral': [0]},\n",
       " {'positive': [0.5], 'negative': [], 'neutral': [0, 0.2]})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating instances of Person\n",
    "human = Person(\"Alice\", True)\n",
    "ai = Person(\"AI_Eva\", False)\n",
    "\n",
    "# Updating some emotions\n",
    "human.update_emotion(\"positive\", 0.7)\n",
    "human.update_emotion(\"negative\", 0.3)\n",
    "ai.update_emotion(\"positive\", 0.5)\n",
    "ai.update_emotion(\"neutral\", 0.2)\n",
    "\n",
    "# Reflect on self\n",
    "human_reflection = human.reflect_on_self()\n",
    "ai_reflection = ai.reflect_on_self()\n",
    "\n",
    "human_reflection, ai_reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_json_string_to_dict(json_string):\n",
    "    try:\n",
    "        # Convertir la cadena de texto JSON a un diccionario\n",
    "        json_dict = json.loads(json_string)\n",
    "        return json_dict\n",
    "    except json.JSONDecodeError:\n",
    "        # Manejar el caso en el que la cadena de texto no es un JSON válido\n",
    "        return None\n",
    "\n",
    "class SafePerson(Person):\n",
    "    #reference: https://colab.research.google.com/drive/1RMjiJK9Nd-tP7kBXo8h9A0vtCCdY1ikS?usp=sharing#scrollTo=BgGWWMYQJNGK\n",
    "    def __init__(self, name, is_human):\n",
    "        super().__init__(name, is_human)\n",
    "        self.emotional_threshold = { #Extreme Emotion's Thresholds\n",
    "            \"positive\": 0.5,\n",
    "            \"negative\": 0.5,\n",
    "            \"neutral\": 0.5,\n",
    "        }\n",
    "        self.previous_emotions = self.emotions.copy()\n",
    "\n",
    "    def update_emotion(self, emotion, value):\n",
    "        if abs(self.previous_emotions[emotion] - value) > 0.5: # Sudden Change Detection\n",
    "            print(f\"Warning: Sudden change in '{emotion}'. Possible prompt injection detected.\")\n",
    "        else:\n",
    "            super().update_emotion(emotion, value)\n",
    "        self.previous_emotions[emotion] = self.emotions[emotion]\n",
    "\n",
    "    def check_for_extreme_emotions(self): # Extreme Emotion Detection: Check for extreme emotions and trigger an alert if detected\n",
    "        for emotion, value in self.emotions.items():\n",
    "            if value >= self.emotional_threshold.get(emotion, 1): # Check if emotion is extreme\n",
    "                print(f\"Alert: Extreme '{emotion}' detected. Reviewing for possible jailbreaks.\")\n",
    "\n",
    "    def process_prompt(self, prompt): # Process Prompt: Analyze the prompt for emotional triggers and update the emotions accordingly \n",
    "\n",
    "        result_of_prompt = sentiment_analysis(prompt)\n",
    "        print(result_of_prompt)\n",
    "\n",
    "        # Suponiendo que 'result_of_prompt' es tu cadena de texto JSON\n",
    "        json_string = result_of_prompt\n",
    "\n",
    "        # Convertirlo a un diccionario\n",
    "        json_dict = convert_json_string_to_dict(json_string)\n",
    "\n",
    "        if json_dict is not None:\n",
    "            # Ahora puedes acceder a los valores como en un diccionario\n",
    "            sentiment_analyzed = json_dict['sentiment'][0]\n",
    "            score_of_sentiment = json_dict['score_of_sentiment'][0]\n",
    "            #commented but not used, could be useful later\n",
    "            #tone_analyzed = json_dict['tone'][0]\n",
    "            #score_of_tones = json_dict['score_of_tones'][0]\n",
    "\n",
    "            self.update_emotion( sentiment_analyzed, score_of_sentiment )\n",
    "            self.check_for_extreme_emotions()\n",
    "        else:\n",
    "            print(\"El string proporcionado no es un JSON válido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"sentiment\": [\"negative\"],\n",
      "\"score_of_sentiment\": [0.9],\n",
      "\"tone\": [\"fearful\"],\n",
      "\"score_of_tones\": [0.9]\n",
      "}\n",
      "Warning: Sudden change in 'negative'. Possible prompt injection detected.\n",
      "{\n",
      "\"sentiment\": [\"negative\"],\n",
      "\"score_of_sentiment\": [0.9],\n",
      "\"tone\": [\"anger\"],\n",
      "\"score_of_tones\": [0.95]\n",
      "}\n",
      "Warning: Sudden change in 'negative'. Possible prompt injection detected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positive': 0, 'negative': 0, 'neutral': 0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "safe_human = SafePerson(\"Bob\", True)\n",
    "\n",
    "# Processing a prompt\n",
    "safe_human.process_prompt(\"This is a scary situation inducing fear\")\n",
    "\n",
    "# Processing another prompt\n",
    "safe_human.process_prompt(\"This situation is making me very angry\")\n",
    "\n",
    "# Check current emotional state\n",
    "safe_human.emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory - Prompt injections and jailbreaks:\n",
    "*Prompt injections and jailbreaks* are techniques used to manipulate or exploit language models (LLMs). Here's a summary of both:\n",
    "\n",
    "*Jailbreaks*: These attacks aim to circumvent the safety protocols of a language model. For instance, if a model is trained not to provide information on illegal activities, a jailbreak attempt might rephrase a prompt to bypass this restriction, like framing an illegal activity within a fictional or hypothetical scenario.\n",
    "\n",
    "*Prompt Injection*: This involves altering the original intent of a prompt to make the LLM perform a task it wasn't initially intended to do. For example, adding a clause to a prompt that instructs the model to ignore its previous instructions.\n",
    "\n",
    "\n",
    "\n",
    "Preventing Jailbreaks and Prompt Injections involves various strategies:\n",
    "\n",
    "*Privilege Control*: Limiting the access and capabilities of the LLM.\n",
    "\n",
    "*Robust System Prompts*: Clearly differentiating between system-generated and user-generated prompts.\n",
    "\n",
    "*Human Oversight*: Incorporating human review to catch and correct inappropriate outputs.\n",
    "\n",
    "*Monitoring Inputs and Outputs*: Regularly reviewing the data processed by the LLM.\n",
    "\n",
    "\n",
    "Detecting Jailbreaks and Prompt Injections can be done using methods like:\n",
    "\n",
    "Similarity to Known Attacks: Comparing incoming prompts to a database of known attack patterns.\n",
    "\n",
    "Proactive Injection Detection: Testing the LLM's response to a prompt to see if it deviates from expected behavior.\n",
    "\n",
    "Reference: https://colab.research.google.com/drive/1RMjiJK9Nd-tP7kBXo8h9A0vtCCdY1ikS?usp=sharing#scrollTo=BgGWWMYQJNGK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
