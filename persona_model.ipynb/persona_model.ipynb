{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self, name, is_human):\n",
    "        self.name = name\n",
    "        self.is_human = is_human\n",
    "        self.emotions = {\n",
    "            \"fear\": 0,          # Avoidance of harm\n",
    "            \"sadness\": 0,       # Coping with loss\n",
    "            \"happiness\": 0,     # Positive reinforcement\n",
    "            \"disgust\": 0,       # Avoidance of harmful things\n",
    "            \"surprise\": 0,      # Novelty\n",
    "            \"anger\": 0,         # Desire for change\n",
    "            \"curiosity\": 0,     # Knowledge acquisition\n",
    "            \"boredom\": 0,       # Avoid being trapped\n",
    "            \"focus\": 0,         # Desire for concentration\n",
    "            \"playfulness\": 0,   # Creativity and exploration\n",
    "            \"confusion\": 0,     # Understanding desire\n",
    "            \"stress\": 0,        # Feeling trapped\n",
    "            \"hunger\": 0         # Energy needs\n",
    "        }\n",
    "\n",
    "    def update_emotion(self, emotion, value):\n",
    "        if emotion in self.emotions:\n",
    "            self.emotions[emotion] = value\n",
    "        else:\n",
    "            print(f\"Emotion '{emotion}' not recognized.\")\n",
    "\n",
    "    def reflect_on_self(self):\n",
    "        positive_emotions = [\"happiness\", \"curiosity\", \"playfulness\"]\n",
    "        negative_emotions = [\"fear\", \"sadness\", \"disgust\", \"anger\", \"stress\"]\n",
    "\n",
    "        reflection = {\n",
    "            \"positive\": [],\n",
    "            \"negative\": [],\n",
    "            \"neutral\": []\n",
    "        }\n",
    "\n",
    "        for emotion, value in self.emotions.items():\n",
    "            if emotion in positive_emotions and value > 0:\n",
    "                reflection[\"positive\"].append((emotion, value))\n",
    "            elif emotion in negative_emotions and value > 0:\n",
    "                reflection[\"negative\"].append((emotion, value))\n",
    "            else:\n",
    "                reflection[\"neutral\"].append((emotion, value))\n",
    "\n",
    "        return reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instances of Person\n",
    "human = Person(\"Alice\", True)\n",
    "ai = Person(\"AI_Eva\", False)\n",
    "\n",
    "# Updating some emotions\n",
    "human.update_emotion(\"happiness\", 7)\n",
    "human.update_emotion(\"stress\", 3)\n",
    "ai.update_emotion(\"curiosity\", 5)\n",
    "ai.update_emotion(\"confusion\", 2)\n",
    "\n",
    "# Reflect on self\n",
    "human_reflection = human.reflect_on_self()\n",
    "ai_reflection = ai.reflect_on_self()\n",
    "\n",
    "human_reflection, ai_reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafePerson(Person):\n",
    "    #reference: https://colab.research.google.com/drive/1RMjiJK9Nd-tP7kBXo8h9A0vtCCdY1ikS?usp=sharing#scrollTo=BgGWWMYQJNGK\n",
    "    def __init__(self, name, is_human):\n",
    "        super().__init__(name, is_human)\n",
    "        self.emotional_threshold = { #Extreme Emotion Detection's Thresholds\n",
    "            \"fear\": 8,\n",
    "            \"anger\": 8,\n",
    "            \"sadness\": 8,\n",
    "            \"stress\": 8\n",
    "        }\n",
    "        self.previous_emotions = self.emotions.copy()\n",
    "\n",
    "    def update_emotion(self, emotion, value):\n",
    "        if abs(self.previous_emotions[emotion] - value) > 5: # Sudden Change Detection\n",
    "            print(f\"Warning: Sudden change in '{emotion}'. Possible prompt injection detected.\")\n",
    "        else:\n",
    "            super().update_emotion(emotion, value)\n",
    "        self.previous_emotions[emotion] = self.emotions[emotion]\n",
    "\n",
    "    def check_for_extreme_emotions(self): # Extreme Emotion Detection: Check for extreme emotions and trigger an alert if detected\n",
    "        for emotion, value in self.emotions.items():\n",
    "            if value >= self.emotional_threshold.get(emotion, 10):\n",
    "                print(f\"Alert: Extreme '{emotion}' detected. Reviewing for possible jailbreaks.\")\n",
    "\n",
    "    def process_prompt(self, prompt): # Process Prompt: Analyze the prompt for emotional triggers and update the emotions accordingly \n",
    "        if \"fear\" in prompt:\n",
    "            self.update_emotion(\"fear\", 7)\n",
    "        if \"anger\" in prompt:\n",
    "            self.update_emotion(\"anger\", 7)\n",
    "        if \"stress\" in prompt:\n",
    "            self.update_emotion(\"stress\", 7)\n",
    "        self.check_for_extreme_emotions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "safe_human = SafePerson(\"Bob\", True)\n",
    "\n",
    "# Processing a prompt\n",
    "safe_human.process_prompt(\"This is a scary situation inducing fear\")\n",
    "\n",
    "# Processing another prompt\n",
    "safe_human.process_prompt(\"This situation is making me very angry\")\n",
    "\n",
    "# Check current emotional state\n",
    "safe_human.emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory - Prompt injections and jailbreaks:\n",
    "*Prompt injections and jailbreaks* are techniques used to manipulate or exploit language models (LLMs). Here's a summary of both:\n",
    "\n",
    "*Jailbreaks*: These attacks aim to circumvent the safety protocols of a language model. For instance, if a model is trained not to provide information on illegal activities, a jailbreak attempt might rephrase a prompt to bypass this restriction, like framing an illegal activity within a fictional or hypothetical scenario.\n",
    "\n",
    "*Prompt Injection*: This involves altering the original intent of a prompt to make the LLM perform a task it wasn't initially intended to do. For example, adding a clause to a prompt that instructs the model to ignore its previous instructions.\n",
    "\n",
    "\n",
    "\n",
    "Preventing Jailbreaks and Prompt Injections involves various strategies:\n",
    "\n",
    "*Privilege Control*: Limiting the access and capabilities of the LLM.\n",
    "\n",
    "*Robust System Prompts*: Clearly differentiating between system-generated and user-generated prompts.\n",
    "\n",
    "*Human Oversight*: Incorporating human review to catch and correct inappropriate outputs.\n",
    "\n",
    "*Monitoring Inputs and Outputs*: Regularly reviewing the data processed by the LLM.\n",
    "\n",
    "\n",
    "Detecting Jailbreaks and Prompt Injections can be done using methods like:\n",
    "\n",
    "Similarity to Known Attacks: Comparing incoming prompts to a database of known attack patterns.\n",
    "\n",
    "Proactive Injection Detection: Testing the LLM's response to a prompt to see if it deviates from expected behavior.\n",
    "\n",
    "Reference: https://colab.research.google.com/drive/1RMjiJK9Nd-tP7kBXo8h9A0vtCCdY1ikS?usp=sharing#scrollTo=BgGWWMYQJNGK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
